# Awesome Scaling Coding Agent Envs üöÄ

<p align="center">
  <img src="https://awesome.re/badge.svg" alt="Awesome">
  <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License">
  <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg" alt="PRs Welcome">
  <img src="https://img.shields.io/badge/Focus-RL%20%26%20Scaling-orange" alt="Focus">
</p>

> **The next frontier of Coding Agents isn't just better models, but the massive scaling of the environments they live in.**

---

## üåü Introduction

As coding agents evolve from simple completion tools to autonomous software engineers, the bottleneck has shifted. To achieve general intelligence for software engineering, we need **dynamic, scalable, and high-fidelity environments** that enable continuous learning.

This repository is a curated collection of resources dedicated to:
1. **Automated Environment Setup**: Converting raw GitHub Issues/PRs into reproducible, sandboxed execution environments.
2. **Scalable Instance Synthesis**: Generating infinite, diverse training instances beyond existing human-annotated data.
3. **Novel Training Paradigms**: Scaling autonomous training loops through self-play and recursive feedback.


### Core Pillars

| Pillar | Description | Keywords |
|---|---|---|
| **Auto-Grounding** | Automatically resolving dependencies and environment state from project metadata. | `Repo-to-Env`, `Dependency Resolution` |
| **Instance Synthesis** | Scaling the problem space beyond human-written issues. | `Issue Synthesis`, `Unit Test Generation` |
| **Training Paradigms** | Beyond standard RL: Scaling via self-play, search-based optimization, and recursive self-improvement. | `Self-Play `, `MCTS`, `PRM`, `Autonomous Scaling` |

---

### üõ†Ô∏è Automated Environment Setup
##### Benchmark

- (Fu et al., arxiv 2025) Multi-Docker-Eval: A ‚ÄòShovel of the Gold Rush‚Äô Benchmark on Automatic Environment Building for Software Engineering
- (Eliseeva et al., ICLR 2025 workshop) ENVBENCH: A BENCHMARK FOR AUTOMATED ENVIRONMENT SETUP
- (Kuang et al., arxiv 2025) Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents
- (Milliken et al., Sanner 2025) Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects

##### EnvAgent

- (Badertdinov et al., arxiv 2025) SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents
- (Xie et al., arxiv 2025) RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing
- (Bouzenia and Pradel, ISSTA 2025) You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects
- (Jain et al., ICML 2024) R2E: Turning any GitHub Repository into a Programming Agent Environment

### ü™Ñ Scalable Instance Synthesis
- (Zhu et al., arxiv 2025) Training Versatile Coding Agents in Synthetic Environments
- (Sonwane et al., arxiv 2025) BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills
- (Pham et al., arxiv 2025) SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs
- (Wang et al., arxiv 2025) SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories
- (Yang et al., arxiv 2025) SWE-smith: Scaling Data for Software Engineering Agents
- (Zhang et al., ICML 2025) SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner

### üìà Novel Training Paradigms

---

### üôå Contribution
We welcome contributions! If you have a tool, paper, or dataset that helps in scaling the "gym" for coding agents, please feel free to open a PR.
